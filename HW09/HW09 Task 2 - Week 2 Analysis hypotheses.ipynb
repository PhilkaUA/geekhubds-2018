{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1. Идентификация пользователей по посещенным веб-страницам\n",
    "\n",
    "# <center>Неделя 2. Подготовка и первичный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка нескольких обучающих выборок для сравнения\n",
    "\n",
    "Пока мы брали последовательности из 10 сайтов, и это было наобум. Давайте сделаем число сайтов в сессии параметром, чтоб в дальнейшем сравнить модели классификации, обученные на разных выборках – с 5, 7, 10 и 15 сайтами в сессии. Более того, пока мы брали по 10 сайтов подряд, без пересечения. Теперь давайте применим идею скользящего окна – сессии будут перекрываться. \n",
    "\n",
    "**Пример**: для длины сессии 10 и ширины окна 7 файл из 30 записей породит не 3 сессии, как раньше (1-10, 11-20, 21-30), а 5 (1-10, 8-17, 15-24, 22-30, 29-30). При этом в предпоследней сессии будет один ноль, а в последней – 8 нолей.\n",
    "\n",
    "Создадим несколько выборок для разных сочетаний параметров длины сессии и ширины окна. Все они представлены в табличке ниже:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">session_length -&gt;<br>window_size <br></th>\n",
    "    <th class=\"tg-031e\">5</th>\n",
    "    <th class=\"tg-031e\">7</th>\n",
    "    <th class=\"tg-031e\">10</th>\n",
    "    <th class=\"tg-031e\">15</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">5</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">7</td>\n",
    "    <td class=\"tg-031e\"></td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">10</td>\n",
    "    <td class=\"tg-031e\"></td>\n",
    "    <td class=\"tg-031e\"></td>\n",
    "    <td class=\"tg-031e\"><font color='green'>v</font></td>\n",
    "    <td class=\"tg-031e\">v</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Итого должно получиться 18 разреженных матриц – указанные в таблице 9 сочетаний параметров формирования сессий для выборок из 10 и 150 пользователей. При этом 2 выборки мы уже сделали в прошлой части, они соответствуют сочетанию параметров: session_length=10, window_size=10, которые помечены в таблице выше галочкой зеленого цвета (done)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию *prepare_sparse_train_set_window*.\n",
    "\n",
    "Аргументы:\n",
    "- *path_to_csv_files* – путь к каталогу с csv-файлами\n",
    "- *site_freq_path* – путь к pickle-файлу с частотным словарем, полученным в 1 части проекта\n",
    "- *session_length* – длина сессии (параметр)\n",
    "- *window_size* – ширина окна (параметр) \n",
    "\n",
    "Функция должна возвращать 2 объекта:\n",
    "- разреженную матрицу *X_sparse* (двухмерная Scipy.sparse.csr_matrix), в которой строки соответствуют сессиям из *session_length* сайтов, а *max(site_id)* столбцов – количеству посещений *site_id* в сессии. \n",
    "- вектор *y* (Numpy array) \"ответов\" в виде ID пользователей, которым принадлежат сессии из *X_sparse*\n",
    "\n",
    "Детали:\n",
    "- Модифицируйте созданную в 1 части функцию *prepare_train_set*\n",
    "- Некоторые сессии могут повторяться – оставьте как есть, не удаляйте дубликаты\n",
    "- Замеряйте время выполнения итераций цикла с помощью *time* из *time*, *tqdm* из *tqdm* или с помощью виджета [log_progress](https://github.com/alexanderkuk/log-progress) ([статья](https://habrahabr.ru/post/276725/) о нем на Хабрахабре)\n",
    "- 150 файлов из *capstone_websites_data/150users/* должны обрабатываться за несколько секунд (в зависимости от входных параметров). Если дольше – не страшно, но знайте, что функцию можно ускорить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# disable warning in Anaconda\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def dict_modify(d1, d2):\n",
    "    '''Adding frequence count (second tuple element) from dict2 to dict1\n",
    "    d1 ->  site_freq_users_all\n",
    "    d2 ->  site_freq_common_users\n",
    "    '''\n",
    "    d1_keys = set(d1.keys())\n",
    "    d2_keys = set(d2.keys())\n",
    "    intersect_keys = d1_keys.intersection(d2_keys)\n",
    "    return {sites : (d1[sites][0], d1[sites][1] + d2[sites][1]) for sites in intersect_keys}\n",
    "\n",
    "\n",
    "def incomplete_condition(user_data_eval, session_length):\n",
    "    '''Condition for determining incomplete session'''\n",
    "    if not user_data_eval.shape[0] % session_length == 0:   \n",
    "        # Fill zeros for incomplete session\n",
    "        for index in range( user_data_eval.shape[0], (user_data_eval.shape[0] // session_length + 1)*session_length ):\n",
    "            user_data_eval.loc[index] = [0, 0]\n",
    "\n",
    "def intersect1d_set(A,B):\n",
    "    '''Intersection elements for lists'''\n",
    "    if not B:\n",
    "        result = []\n",
    "    else: result = list( set.intersection(set(B),set(A)) )    \n",
    "    return result\n",
    "\n",
    "\n",
    "def sparse_csr(array2D):\n",
    "    '''\n",
    "    make data, indices, indptr for sparse matrix\n",
    "    array2D - input array'''\n",
    "    data = []\n",
    "    indices = []\n",
    "    indptr = [0]\n",
    "    for array in array2D:\n",
    "        unique, counts = np.unique(array[array != 0], return_counts=True)\n",
    "        indptr.append(indptr[-1] + len(unique))\n",
    "        for u, c in zip(unique, counts):\n",
    "            indices.append(u - 1)\n",
    "            data.append(c)\n",
    "    return data, indices, indptr\n",
    "\n",
    "\n",
    "def setdiff1d_modify(A,B):\n",
    "    '''Differets elements of lists'''\n",
    "    if not B:\n",
    "        result = A\n",
    "    else: result = np.array( list( set(A) - set(B) ), dtype=object )\n",
    "    return result\n",
    "\n",
    "\n",
    "def window_session_items(session_length, window_length, array_size):\n",
    "    '''index of windowed session length'''\n",
    "    \n",
    "    # first index\n",
    "    start_index = (session_length + 1) - (session_length - window_length)\n",
    "    tmp_start = start_index\n",
    "    indexs = [1]\n",
    "    indexs.append(start_index)\n",
    "    while tmp_start < array_size:   \n",
    "        tmp_start =  tmp_start +  window_length\n",
    "        indexs.append(tmp_start)\n",
    "    first_index = indexs[:-1]\n",
    "\n",
    "        # second index\n",
    "    step_index = session_length - start_index\n",
    "    second_index = [session_length]\n",
    "    tmp_end = [*map(lambda x: x + step_index, first_index[2:])]\n",
    "    tmp_end = [*map(lambda x: x if x < array_size else array_size, tmp_end)]\n",
    "    second_index = second_index + tmp_end\n",
    "    second_index.append(array_size)\n",
    "\n",
    "    return zip(first_index, second_index)\n",
    "\n",
    "def prepare_sparse_train_set_window(path_to_csv_files, path_pckl_file, session_length=10, window_length=7):\n",
    "    \n",
    "    # Str numeration for site\n",
    "    site_numeration = ['site' + str(index + 1) for index in range(session_length)]\n",
    "\n",
    "    # Inicnjdhtial dataframe\n",
    "    resultData_all = pd.DataFrame(columns=(site_numeration + ['user_id']))\n",
    "    # Initial unique site and index for them\n",
    "    unique_site_all = []\n",
    "    index_list_all = []\n",
    "    site_freq_users_all = dict()\n",
    "    user_id = 0\n",
    "    start_index = 0\n",
    "\n",
    "    def adding_algorytm(user_data):\n",
    "        '''adding sessions to the dataframe'''\n",
    "    \n",
    "        nonlocal user_id, resultData_all\n",
    "        #User id value\n",
    "        user_id = user_id + 1 \n",
    "        # Copy of daraframe for main algorytm\n",
    "        user_data_eval = user_data.copy() \n",
    "       \n",
    "        # Condition for determining incomplete session\n",
    "        incomplete_condition(user_data_eval, session_length)\n",
    "    \n",
    "        # Main alhortm of replacing str sites for numbers\n",
    "        slice_index = [*map(lambda x: (x[0] - 1, x[1] - 1),\\\n",
    "                        window_session_items(session_length, window_length, user_data_eval['site'].shape[0]))]\n",
    "        sessions_length = [*map(lambda x: x[1] - x[0] + 1, slice_index)]\n",
    "        #sessions_range = [*map(np.arange, sessions_length)]\n",
    "\n",
    "        all_sessions = [*map(lambda x: user_data_eval['site'].values[x[0]:x[1]+1], slice_index)]\n",
    "        sessions = len(sessions_length)\n",
    "\n",
    "        resultData = pd.DataFrame(data=all_sessions, index=np.arange(sessions), columns=site_numeration)\n",
    "        resultData = resultData.fillna(0)\n",
    "\n",
    "        # resultData add user id\n",
    "        resultData['user_id'] = pd.Series(user_id, index=resultData.index)\n",
    "    \n",
    "        # Store to the global value\n",
    "        resultData_all = resultData_all.append(resultData, ignore_index=True)\n",
    "        \n",
    "    def path_to_csv(path_to_csv_files, PATH_TO_PROJECT='~/geekhubds/HW09'):\n",
    "        ''' Path to data - csv files'''\n",
    "        # File operations\n",
    "        file_quant = len(glob(path_to_csv_files))\n",
    "        file_names = [file for file in glob(path_to_csv_files)]\n",
    "        file_length = len(file_names)\n",
    "        # Import csv files\n",
    "        user_data = [pd.read_csv(os.path.join(PATH_TO_PROJECT,file)) for file in file_names]\n",
    "        return user_data\n",
    "    \n",
    "    def path_to_plc(path_pckl_file):\n",
    "        ''' Path to data - pckl file'''\n",
    "        with open(path_pckl_file, 'rb') as site_freq:\n",
    "            site_freq = pickle.load(site_freq)\n",
    "        def replace_func(site_freq):\n",
    "            '''Creation replace dic from site freq dictionary'''\n",
    "            return {site:site_freq.get(site)[0] for site in site_freq.keys()}\n",
    "        \n",
    "        return replace_func(site_freq)\n",
    "\n",
    "   \n",
    "    # Main algorytm\n",
    "\n",
    "    # Import csv files \n",
    "    user_data = path_to_csv(path_to_csv_files) \n",
    "\n",
    "    # Import replace dictionary\n",
    "    replasment = path_to_plc(path_pckl_file)\n",
    "            \n",
    "    # Replacing str sites for numbers\n",
    "    [*map(adding_algorytm, user_data)];\n",
    "\n",
    "    #Replacing site_id in column and delete NANs\n",
    "    for site in site_numeration:\n",
    "        resultData_all[site] = resultData_all[site].map(replasment.get)\n",
    "    resultData_all = resultData_all.fillna(0).astype(int)\n",
    "    \n",
    "    # crs matrix algorytm\n",
    "    X_users, y_users = resultData_all.iloc[:, :-1].values, resultData_all.iloc[:, -1].values\n",
    "    \n",
    "    data_users, indices_users, indptr_users = sparse_csr(X_users)\n",
    "    X_sparse_users = csr_matrix((data_users, indices_users, indptr_users), dtype=int)\n",
    "    \n",
    "    return X_sparse_users, y_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_to_csv_files = '3users\\*'\n",
    "path_pckl_file = 'site_freq_3users.pkl'\n",
    "X_sparse_users, y_users =  \\\n",
    "                prepare_sparse_train_set_window(path_to_csv_files, path_pckl_file, session_length=10, window_length=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 3, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 0, 4, 0, 1, 0, 0, 0, 2, 1],\n",
       "        [0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_users.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените полученную функцию с параметрами *session_length=5* и *window_size=3* к игрушечному примеру. Убедитесь, что все работает как надо.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv_files = '3users\\*'\n",
    "path_pckl_file = 'site_freq_3users.pkl'\n",
    "X_toy_s5_w3, y_s5_w3 =  prepare_sparse_train_set_window(path_to_csv_files, path_pckl_file, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy_s5_w3.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s5_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запустите созданную функцию 16 раз с помощью циклов по числу пользователей num_users (10 или 150), значениям параметра *session_length* (15, 10, 7 или 5) и значениям параметра *window_size* (10, 7 или 5). Сериализуйте все 16 разреженных матриц (обучающие выборки) и векторов (метки целевого класса – ID пользователя) в файлы `X_sparse_{num_users}users_s{session_length}_w{window_size}.pkl` и `y_{num_users}users_s{session_length}_w{window_size}.pkl`.**\n",
    "\n",
    "**Чтоб убедиться, что мы все далее будем работать с идентичными объектами, запишите в список *data_lengths* число строк во всех полученных рареженных матрицах (16 значений). Если какие-то будут совпадать, это нормально (можно сообразить, почему).**\n",
    "\n",
    "**На моем ноутбуке этот участок кода отработал за 26 секунд, хотя понятно, что все зависит от эффективности реализации функции *prepare_sparse_train_set_window* и мощности используемого железа. И честно говоря, моя первая реализация была намного менее эффективной (34 минуты), так что тут у Вас есть возможность оптимизировать свой код.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:10 session:15 window:10 matrix length:14065\n",
      "Users:10 session:15 window:7 matrix length:20093\n",
      "Users:10 session:10 window:7 matrix length:20090\n",
      "Users:10 session:7 window:7 matrix length:20087\n",
      "Users:10 session:15 window:5 matrix length:28125\n",
      "Users:10 session:10 window:5 matrix length:28122\n",
      "Users:10 session:7 window:5 matrix length:28124\n",
      "Users:10 session:5 window:5 matrix length:28118\n",
      "Users:150 session:15 window:10 matrix length:137094\n",
      "Users:150 session:15 window:7 matrix length:195821\n",
      "Users:150 session:10 window:7 matrix length:195785\n",
      "Users:150 session:7 window:7 matrix length:195712\n",
      "Users:150 session:15 window:5 matrix length:274098\n",
      "Users:150 session:10 window:5 matrix length:274038\n",
      "Users:150 session:7 window:5 matrix length:274027\n",
      "Users:150 session:5 window:5 matrix length:273957\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_lengths_dic = []\n",
    "data_lengths = []\n",
    "\n",
    "#inputdata\n",
    "num_users = (('10users\\*', 'site_freq_10users.pkl'),('150users\\*', 'site_freq_150users.pkl'))\n",
    "\n",
    "\n",
    "for users in num_users:\n",
    "    \n",
    "    # User number\n",
    "    number_users = int(re.findall(r'\\d+', users[0])[0])\n",
    "    \n",
    "    for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]):\n",
    "        \n",
    "        if window_size <= session_length and (window_size, session_length) != (10, 10):\n",
    "            \n",
    "            X_sparse, y = prepare_sparse_train_set_window(users[0], users[1], session_length, window_size)\n",
    "            \n",
    "            data_lengths_dic.append({(session_length,window_size):X_sparse.shape[0]})\n",
    "            data_lengths.append(X_sparse.shape[0])\n",
    "            \n",
    "            #store pickle files\n",
    "            with open(f'X_sparse_{number_users}users_s{session_length}_w{window_size}.pkl', 'wb') as X_pkl:\n",
    "                pickle.dump(X_sparse, X_pkl, protocol=2)\n",
    "            with open(f'y_{number_users}users_s{session_length}_w{window_size}.pkl', 'wb') as y_pkl:\n",
    "                pickle.dump(y, y_pkl, protocol=2)\n",
    "            \n",
    "            print(f'Users:{number_users} session:{session_length} window:{window_size} matrix length:{X_sparse.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Вопрос 1. </font>Сколько всего уникальных значений в списке `data_lengths`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unique values in the data_lengths list: 16\n"
     ]
    }
   ],
   "source": [
    "print('   Unique values in the data_lengths list:', len( np.unique(data_lengths) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second realization without import pickle file\n",
    "# # Functions\n",
    "\n",
    "# def dict_modify(d1, d2):\n",
    "#     '''Adding frequence count (second tuple element) from dict2 to dict1\n",
    "#     d1 ->  site_freq_users_all\n",
    "#     d2 ->  site_freq_common_users\n",
    "#     '''\n",
    "#     d1_keys = set(d1.keys())\n",
    "#     d2_keys = set(d2.keys())\n",
    "#     intersect_keys = d1_keys.intersection(d2_keys)\n",
    "#     return {sites : (d1[sites][0], d1[sites][1] + d2[sites][1]) for sites in intersect_keys}\n",
    "\n",
    "\n",
    "# def incomplete_condition(user_data_eval, session_length):\n",
    "#     '''Condition for determining incomplete session'''\n",
    "#     if not user_data_eval.shape[0] % session_length == 0:   \n",
    "#         # Fill zeros for incomplete session\n",
    "#         for index in range( user_data_eval.shape[0], (user_data_eval.shape[0] // session_length + 1)*session_length ):\n",
    "#             user_data_eval.loc[index] = [0, 0]\n",
    "\n",
    "# def intersect1d_set(A,B):\n",
    "#     '''Intersection elements for lists'''\n",
    "#     if not B:\n",
    "#         result = []\n",
    "#     else: result = list( set.intersection(set(B),set(A)) )    \n",
    "#     return result\n",
    "\n",
    "\n",
    "# def sparse_csr(array2D):\n",
    "#     '''\n",
    "#     make data, indices, indptr for sparse matrix\n",
    "#     array2D - input array'''\n",
    "#     data = []\n",
    "#     indices = []\n",
    "#     indptr = [0]\n",
    "#     for array in array2D:\n",
    "#         unique, counts = np.unique(array[array != 0], return_counts=True)\n",
    "#         indptr.append(indptr[-1] + len(unique))\n",
    "#         for u, c in zip(unique, counts):\n",
    "#             indices.append(u - 1)\n",
    "#             data.append(c)\n",
    "#     return data, indices, indptr\n",
    "\n",
    "\n",
    "# def setdiff1d_modify(A,B):\n",
    "#     '''Differets elements of lists'''\n",
    "#     if not B:\n",
    "#         result = A\n",
    "#     else: result = np.array( list( set(A) - set(B) ), dtype=object )\n",
    "#     return result\n",
    "\n",
    "\n",
    "# def window_session_items(session_length, window_length, array_size):\n",
    "#     '''index of windowed session length'''\n",
    "    \n",
    "#     # first index\n",
    "#     start_index = (session_length + 1) - (session_length - window_length)\n",
    "#     tmp_start = start_index\n",
    "#     indexs = [1]\n",
    "#     indexs.append(start_index)\n",
    "#     while tmp_start < array_size:   \n",
    "#         tmp_start =  tmp_start +  window_length\n",
    "#         indexs.append(tmp_start)\n",
    "#     first_index = indexs[:-1]\n",
    "\n",
    "#         # second index\n",
    "#     step_index = session_length - start_index\n",
    "#     second_index = [session_length]\n",
    "#     tmp_end = [*map(lambda x: x + step_index, first_index[2:])]\n",
    "#     tmp_end = [*map(lambda x: x if x < array_size else array_size, tmp_end)]\n",
    "#     second_index = second_index + tmp_end\n",
    "#     second_index.append(array_size)\n",
    "\n",
    "#     return zip(first_index, second_index)\n",
    "\n",
    "# def prepare_sparse_train_set_window(path_to_csv_files, session_length=10, window_length=7):\n",
    "    \n",
    "#     # Str numeration for site\n",
    "#     site_numeration = ['site' + str(index + 1) for index in range(session_length)]\n",
    "\n",
    "#     # Inicnjdhtial dataframe\n",
    "#     resultData_all = pd.DataFrame(columns=(site_numeration + ['user_id']))\n",
    "#     # Initial unique site and index for them\n",
    "#     unique_site_all = []\n",
    "#     index_list_all = []\n",
    "#     site_freq_users_all = dict()\n",
    "#     user_id = 0\n",
    "#     start_index = 0\n",
    "\n",
    "#     def adding_algorytm(user_data):\n",
    "#         '''adding sessions to the dataframe'''\n",
    "    \n",
    "#         nonlocal user_id, resultData_all\n",
    "#         #User id value\n",
    "#         user_id = user_id + 1 \n",
    "#         # Copy of daraframe for main algorytm\n",
    "#         user_data_eval = user_data.copy() \n",
    "       \n",
    "#         # Condition for determining incomplete session\n",
    "#         incomplete_condition(user_data_eval, session_length)\n",
    "    \n",
    "#         # Main alhortm of replacing str sites for numbers\n",
    "#         slice_index = [*map(lambda x: (x[0] - 1, x[1] - 1),\\\n",
    "#                         window_session_items(session_length, window_length, user_data_eval['site'].shape[0]))]\n",
    "#         sessions_length = [*map(lambda x: x[1] - x[0] + 1, slice_index)]\n",
    "#         #sessions_range = [*map(np.arange, sessions_length)]\n",
    "\n",
    "#         all_sessions = [*map(lambda x: user_data_eval['site'].values[x[0]:x[1]+1], slice_index)]\n",
    "#         sessions = len(sessions_length)\n",
    "\n",
    "#         resultData = pd.DataFrame(data=all_sessions, index=np.arange(sessions), columns=site_numeration)\n",
    "#         resultData = resultData.fillna(0)\n",
    "\n",
    "#         # resultData add user id\n",
    "#         resultData['user_id'] = pd.Series(user_id, index=resultData.index)\n",
    "    \n",
    "#         # Store to the global value\n",
    "#         resultData_all = resultData_all.append(resultData, ignore_index=True)\n",
    "        \n",
    "#     def path_to_csv(path_to_csv_files, PATH_TO_PROJECT='~/geekhubds/HW09'):\n",
    "#         ''' Path to data - csv files'''\n",
    "#         # File operations\n",
    "#         file_quant = len(glob(path_to_csv_files))\n",
    "#         file_names = [file for file in glob(path_to_csv_files)]\n",
    "#         file_length = len(file_names)\n",
    "#         # Import csv files\n",
    "#         user_data = [pd.read_csv(os.path.join(PATH_TO_PROJECT,file)) for file in file_names]\n",
    "#         return user_data#, file_length\n",
    "   \n",
    "#     def vocabruary(user_data):\n",
    "#         '''make vocabruary -> {site: (index,counts)}'''\n",
    "    \n",
    "#         nonlocal unique_site_all,index_list_all,site_freq_users_all,start_index\n",
    "\n",
    "#         # Unique sites and them counts\n",
    "#         unique_counts = user_data.groupby(['site'], sort=False).size()\n",
    "#         unique_site = np.array(unique_counts.index)\n",
    "#         #unique_counts = np.array(unique_counts)\n",
    "\n",
    "#        # Intersect of unique sites (indexes) for each iteration \n",
    "#         common_site = intersect1d_set(unique_site, unique_site_all)\n",
    "#         index_com_site = np.where(np.in1d(unique_site, common_site))[0]\n",
    "    \n",
    "#         # Difference of unique sites (indexes) for each iteration  \n",
    "#         not_common_site = setdiff1d_modify(unique_site, unique_site_all)\n",
    "#         index_not_common_site = np.where(np.in1d(unique_site, not_common_site))[0]\n",
    "    \n",
    "#         # Store to global\n",
    "#         unique_site_all = unique_site_all + not_common_site.tolist()\n",
    "#         lenght_unique = not_common_site.shape[0]\n",
    "    \n",
    "#         # Index of unique sites for each loop and store to global index value\n",
    "#         index_list = np.arange(start_index + 1, start_index + lenght_unique + 1)\n",
    "#         start_index = start_index + lenght_unique\n",
    "#         index_list_all = index_list_all + index_list.tolist() \n",
    "    \n",
    "#         # Unique elements for each iteration\n",
    "#         count_index = map(lambda x: x, index_list)\n",
    "#         counts_sites = map(lambda x: unique_counts[x], index_not_common_site)\n",
    "#         freq_site =  zip(count_index, counts_sites)\n",
    "    \n",
    "#         # Store to dictionary\n",
    "#         site_freq_users = dict(zip(unique_site[index_not_common_site].tolist(),freq_site))\n",
    "#         site_freq_users_all.update(site_freq_users)\n",
    "\n",
    "#         # Common unique elements for each iteration and store ti dictionary\n",
    "#         count_comon_index = [site_freq_users_all.get(i)[0] for i in common_site]\n",
    "#         counts_comon_sites = unique_counts[index_com_site].tolist()\n",
    "#         freq_common_site = zip(count_comon_index, counts_comon_sites)\n",
    "#         site_freq_common_users = dict(zip(common_site, freq_common_site))\n",
    "#         site_freq_users_all.update(dict_modify(site_freq_users_all, site_freq_common_users))\n",
    "   \n",
    "#     # Main algorytm\n",
    "\n",
    "#     # Import csv files \n",
    "#     user_data = path_to_csv(path_to_csv_files) \n",
    "\n",
    "#     # Dictionary algorytm\n",
    "#     [*map(vocabruary,user_data)];\n",
    "#     # Replacing index and raplace dictionary\n",
    "#     replasment = dict(zip(unique_site_all, index_list_all))\n",
    "            \n",
    "#     # Replacing str sites for numbers\n",
    "#     [*map(adding_algorytm, user_data)];\n",
    "\n",
    "#     #Replacing site_id in column and delete NANs\n",
    "#     for site in site_numeration:\n",
    "#         resultData_all[site] = resultData_all[site].map(replasment.get)\n",
    "#     resultData_all = resultData_all.fillna(0).astype(int)\n",
    "    \n",
    "#     # crs matrix algorytm\n",
    "#     X_users, y_users = resultData_all.iloc[:, :-1].values, resultData_all.iloc[:, -1].values\n",
    "    \n",
    "#     data_users, indices_users, indptr_users = sparse_csr(X_users)\n",
    "#     X_sparse_users = csr_matrix((data_users, indices_users, indptr_users), dtype=int)\n",
    "    \n",
    "#     return X_sparse_users, y_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The 1 realisation of the function from the week 2 \n",
    "\n",
    "# def prepare_train_set(path_to_csv_files, session_length=10, window_length=7):\n",
    " \n",
    "#     # Str numeration for site\n",
    "#     site_numeration = ['site' + str(index + 1) for index in range(session_length)]\n",
    "\n",
    "#     # Inicnjdhtial dataframe\n",
    "#     resultData_all = pd.DataFrame(columns=(site_numeration + ['user_id']))\n",
    "#     # Initial unique site and index for them\n",
    "#     #unique_site_all = []\n",
    "#     index_list_all = []\n",
    "#     #site_freq_users_all = dict()\n",
    "#     all_time = [] \n",
    "#     start_index = 0\n",
    "\n",
    "#     # Functions\n",
    "    \n",
    "#     def path_to_csv(path_to_csv_files, PATH_TO_PROJECT='~/geekhubds/HW09'):\n",
    "#         ''' Path to data - csv files'''\n",
    "#         # File operations\n",
    "#         file_quant = len(glob(path_to_csv_files))\n",
    "#         file_names = [file for file in glob(path_to_csv_files)]\n",
    "#         file_length = len(file_names)\n",
    "#         # Import csv files\n",
    "#         user_data = [pd.read_csv(os.path.join(PATH_TO_PROJECT,file)) for file in file_names]\n",
    "#         return user_data, file_length\n",
    "\n",
    "#     def dict_modify(d1, d2):\n",
    "#         '''Adding frequence count (second tuple element) from dict2 to dict1\n",
    "#         d1 ->  site_freq_users_all\n",
    "#         d2 ->  site_freq_common_users\n",
    "#         '''\n",
    "#         d1_keys = set(d1.keys())\n",
    "#         d2_keys = set(d2.keys())\n",
    "#         intersect_keys = d1_keys.intersection(d2_keys)\n",
    "#         return {sites : (d1[sites][0], d1[sites][1] + d2[sites][1]) for sites in intersect_keys}\n",
    "\n",
    "#     def intersect1d_set(A,B):\n",
    "#         '''Intersection elements for lists'''\n",
    "#         if not B:\n",
    "#             result = []\n",
    "#         else: result = list( set.intersection(set(B),set(A)) )    \n",
    "#         return result\n",
    "\n",
    "#     def setdiff1d_modify(A,B):\n",
    "#         '''Differets elements of lists'''\n",
    "#         if not B:\n",
    "#             result = A\n",
    "#         else: result = np.array( list( set(A) - set(B) ), dtype=object )\n",
    "#         return result\n",
    "\n",
    "#     def incomplete_condition(user_data_eval, session_length):\n",
    "#         '''Condition for determining incomplete session'''\n",
    "#         if not user_data_eval.shape[0] % session_length == 0:   \n",
    "#             # Fill zeros for incomplete session\n",
    "#             for index in range( user_data_eval.shape[0], (user_data_eval.shape[0] // session_length + 1)*session_length ):\n",
    "#                 user_data_eval.loc[index] = [0, 0]\n",
    "\n",
    "#     def window_session_items(session_length, window_length, array_size):\n",
    "#         '''index of windowed session length'''\n",
    "    \n",
    "#         # first index\n",
    "#         start_index = (session_length + 1) - (session_length - window_length)\n",
    "#         tmp_start = start_index\n",
    "#         indexs = [1]\n",
    "#         indexs.append(start_index)\n",
    "#         while tmp_start < array_size:   \n",
    "#             tmp_start =  tmp_start +  window_length\n",
    "#             indexs.append(tmp_start)\n",
    "#         first_index = indexs[:-1]\n",
    "\n",
    "#         # second index\n",
    "#         step_index = session_length - start_index\n",
    "#         second_index = [session_length]\n",
    "#         tmp_end = [*map(lambda x: x + step_index, first_index[2:])]\n",
    "#         tmp_end = [*map(lambda x: x if x < array_size else array_size, tmp_end)]\n",
    "#         second_index = second_index + tmp_end\n",
    "#         second_index.append(array_size)\n",
    "\n",
    "#         return zip(first_index, second_index)\n",
    "\n",
    "#     user_data, file_length = path_to_csv(path_to_csv_files)\n",
    "           \n",
    "#     for file in np.arange(file_length):\n",
    " \n",
    "#         # Unique sites and them counts\n",
    "#         unique_counts = user_data[file].groupby(['site'], sort=False).size()\n",
    "#         unique_site = np.array(unique_counts.index)\n",
    "#         unique_counts = np.array(unique_counts)\n",
    "\n",
    "#         # Intersect of unique sites (indexes) for each iteration \n",
    "#         common_site = intersect1d_set(unique_site, unique_site_all)\n",
    "#         index_com_site = np.where(np.in1d(unique_site, common_site))[0]\n",
    "\n",
    "#         # Difference of unique sites (indexes) for each iteration  \n",
    "#         not_common_site = setdiff1d_modify(unique_site, unique_site_all)\n",
    "#         index_not_common_site = np.where( np.in1d( unique_site, not_common_site ))[0]\n",
    "\n",
    "#         # Store to global\n",
    "#         unique_site_all = unique_site_all + not_common_site.tolist()\n",
    "#         lenght_unique = not_common_site.shape[0]\n",
    "\n",
    "#         # Index of unique sites for each loop and store to global index value\n",
    "#         index_list = np.arange(start_index + 1, start_index + lenght_unique + 1)\n",
    "#         start_index = start_index + lenght_unique\n",
    "#         index_list_all = index_list_all + index_list.tolist() \n",
    "\n",
    "#         # Unique elements for each iteration\n",
    "#         count_index = map(lambda x: x, index_list)\n",
    "#         counts_sites = map(lambda x: unique_counts[x], index_not_common_site)\n",
    "#         freq_site = list( zip(count_index, counts_sites) )\n",
    "#         # Store to dictionary\n",
    "#         site_freq_users = dict(zip(unique_site[index_not_common_site].tolist(),freq_site))\n",
    "#         site_freq_users_all.update(site_freq_users)\n",
    "\n",
    "#         # Common unique elements for each iteration and store ti dictionary\n",
    "#         count_comon_index = [site_freq_users_all.get(i)[0] for i in common_site]\n",
    "#         counts_comon_sites = unique_counts[index_com_site].tolist()\n",
    "#         freq_common_site = list( zip(count_comon_index, counts_comon_sites) )\n",
    "#         site_freq_common_users = dict(zip(common_site, freq_common_site))\n",
    "#         site_freq_users_all.update(dict_modify(site_freq_users_all, site_freq_common_users))\n",
    "\n",
    "#         # Replacing index and raplace dictionary\n",
    "#         replasment = dict(zip(unique_site_all,index_list_all))\n",
    "    \n",
    "#         # User id value\n",
    "#         user_id = file + 1 \n",
    "#         # Copy of daraframe for main algorytm\n",
    "#         user_data_eval = user_data[file].copy() \n",
    "    \n",
    "#         # Condition for determining incomplete session\n",
    "#         incomplete_condition(user_data_eval, session_length)\n",
    "    \n",
    "#         # Main alhortm of replacing str sites for numbers\n",
    "#         slice_index = [*map(lambda x: (x[0] - 1, x[1] - 1),\\\n",
    "#                             window_session_items(session_length, window_length, user_data_eval['site'].shape[0]))]\n",
    "#         sessions_length = map(lambda x: x[1] - x[0] + 1, slice_index)\n",
    "#         sessions_range = [*map(np.arange, sessions_length)]\n",
    "\n",
    "#         all_sessions = [*map(lambda x: user_data_eval['site'].values[x[0]:x[1]+1], slice_index)]\n",
    "#         sessions = len([*window_session_items(session_length, window_length, user_data_eval['site'].shape[0])])\n",
    "\n",
    "#         resultData = pd.DataFrame(index=np.arange(sessions), columns=site_numeration)\n",
    "\n",
    "#         for session in np.arange(sessions):\n",
    "#             for index in sessions_range[session]:\n",
    "#                 resultData.iat[session,index] = all_sessions[session].tolist()[index]\n",
    "#         resultData = resultData.fillna(0)     \n",
    "\n",
    "#         # resultData add user id\n",
    "#         resultData['user_id'] = pd.Series(user_id, index=resultData.index)\n",
    "    \n",
    "#         # Store to the global value\n",
    "#         resultData_all = resultData_all.append(resultData, ignore_index=True)\n",
    "    \n",
    "#     #Replacing site_id in column and delete NANs\n",
    "#     for site in site_numeration:\n",
    "#         resultData_all[site] = resultData_all[site].map(replasment.get)\n",
    "#     resultData_all = resultData_all.fillna(0).astype(int)\n",
    "\n",
    "#     return resultData_all, site_freq_users_all\n",
    "\n",
    "\n",
    "# #     def vocabruary_index(user_data):\n",
    "\n",
    "# #         nonlocal unique_site_all, index_list_all, site_freq_users_all, start_index\n",
    "    \n",
    "# #         # Unique sites and them counts\n",
    "# #         unique_counts = user_data.groupby(['site'], sort=False).size()\n",
    "# #         unique_site = np.array(unique_counts.index)\n",
    "\n",
    "# #        # Intersect of unique sites (indexes) for each iteration \n",
    "# #         common_site = intersect1d_set(unique_site, unique_site_all)\n",
    "# #         index_com_site = np.where(np.in1d(unique_site, common_site))[0]\n",
    "    \n",
    "# #         # Difference of unique sites (indexes) for each iteration  \n",
    "# #         not_common_site = setdiff1d_modify(unique_site, unique_site_all)\n",
    "# #         index_not_common_site = np.where(np.in1d(unique_site, not_common_site))[0]\n",
    "\n",
    "    \n",
    "# #         # Store to global\n",
    "# #         unique_site_all = unique_site_all + not_common_site.tolist()\n",
    "# #         lenght_unique = not_common_site.shape[0]\n",
    "    \n",
    "# #         index_list = np.arange(start_index + 1, start_index + lenght_unique + 1)\n",
    "# #         start_index = start_index + lenght_unique\n",
    "# #         index_list_all = index_list_all + index_list.tolist() \n",
    "    \n",
    "# #         # Unique elements for each iteration\n",
    "# #         count_index = map(lambda x: x, index_list)\n",
    "    \n",
    "# #         # Store to dictionary\n",
    "# #         site_freq_users = dict(zip(unique_site[index_not_common_site].tolist(),count_index))\n",
    "# #         site_freq_users_all.update(site_freq_users)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The third realisation of the function from the week 1\n",
    "# def prepare_train_set(path_to_csv_files, session_length=10, window_length=7):\n",
    " \n",
    "#     # Str numeration for site\n",
    "#     site_numeration = ['site' + str(index + 1) for index in range(session_length)]\n",
    "\n",
    "#     # Inicnjdhtial dataframe\n",
    "#     resultData_all = pd.DataFrame(columns=(site_numeration + ['user_id']))\n",
    "#     # Initial unique site and index for them\n",
    "#     unique_site_all = []\n",
    "#     index_list_all = []\n",
    "#     site_freq_users_all = dict()\n",
    "#     all_time = [] \n",
    "#     start_index = 0\n",
    "\n",
    "#     # Functions\n",
    "    \n",
    "#     def path_to_csv(path_to_csv_files, PATH_TO_PROJECT='~/geekhubds/HW09'):\n",
    "#         ''' Path to data - csv files'''\n",
    "#         # File operations\n",
    "#         file_quant = len(glob(path_to_csv_files))\n",
    "#         file_names = [file for file in glob(path_to_csv_files)]\n",
    "#         file_length = len(file_names)\n",
    "#         # Import csv files\n",
    "#         user_data = [pd.read_csv(os.path.join(PATH_TO_PROJECT,file)) for file in file_names]\n",
    "#         return user_data, file_length\n",
    "\n",
    "#     def dict_modify(d1, d2):\n",
    "#         '''Adding frequence count (second tuple element) from dict2 to dict1\n",
    "#         d1 ->  site_freq_users_all\n",
    "#         d2 ->  site_freq_common_users\n",
    "#         '''\n",
    "#         d1_keys = set(d1.keys())\n",
    "#         d2_keys = set(d2.keys())\n",
    "#         intersect_keys = d1_keys.intersection(d2_keys)\n",
    "#         return {sites : (d1[sites][0], d1[sites][1] + d2[sites][1]) for sites in intersect_keys}\n",
    "\n",
    "#     def intersect1d_set(A,B):\n",
    "#         '''Intersection elements for lists'''\n",
    "#         if not B:\n",
    "#             result = []\n",
    "#         else: result = list( set.intersection(set(B),set(A)) )    \n",
    "#         return result\n",
    "\n",
    "#     def setdiff1d_modify(A,B):\n",
    "#         '''Differets elements of lists'''\n",
    "#         if not B:\n",
    "#             result = A\n",
    "#         else: result = np.array( list( set(A) - set(B) ), dtype=object )\n",
    "#         return result\n",
    "\n",
    "#     def incomplete_condition(user_data_eval, session_length):\n",
    "#         '''Condition for determining incomplete session'''\n",
    "#         if not user_data_eval.shape[0] % session_length == 0:   \n",
    "#             # Fill zeros for incomplete session\n",
    "#             for index in range( user_data_eval.shape[0], (user_data_eval.shape[0] // session_length + 1)*session_length ):\n",
    "#                 user_data_eval.loc[index] = [0, 0]\n",
    "\n",
    "#     def window_session_items(session_length, window_length, array_size):\n",
    "#         '''index of windowed session length'''\n",
    "    \n",
    "#         # first index\n",
    "#         start_index = (session_length + 1) - (session_length - window_length)\n",
    "#         tmp_start = start_index\n",
    "#         indexs = [1]\n",
    "#         indexs.append(start_index)\n",
    "#         while tmp_start < array_size:   \n",
    "#             tmp_start =  tmp_start +  window_length\n",
    "#             indexs.append(tmp_start)\n",
    "#         first_index = indexs[:-1]\n",
    "\n",
    "#         # second index\n",
    "#         step_index = session_length - start_index\n",
    "#         second_index = [session_length]\n",
    "#         tmp_end = [*map(lambda x: x + step_index, first_index[2:])]\n",
    "#         tmp_end = [*map(lambda x: x if x < array_size else array_size, tmp_end)]\n",
    "#         second_index = second_index + tmp_end\n",
    "#         second_index.append(array_size)\n",
    "\n",
    "#         return zip(first_index, second_index)\n",
    "\n",
    "#     user_data, file_length = path_to_csv(path_to_csv_files)\n",
    "           \n",
    "#     for file in np.arange(file_length):\n",
    " \n",
    "#         # Unique sites and them counts\n",
    "#         unique_counts = user_data[file].groupby(['site'], sort=False).size()\n",
    "#         unique_site = np.array(unique_counts.index)\n",
    "#         unique_counts = np.array(unique_counts)\n",
    "\n",
    "#         # Intersect of unique sites (indexes) for each iteration \n",
    "#         common_site = intersect1d_set(unique_site, unique_site_all)\n",
    "#         index_com_site = np.where(np.in1d(unique_site, common_site))[0]\n",
    "\n",
    "#         # Difference of unique sites (indexes) for each iteration  \n",
    "#         not_common_site = setdiff1d_modify(unique_site, unique_site_all)\n",
    "#         index_not_common_site = np.where( np.in1d( unique_site, not_common_site ))[0]\n",
    "\n",
    "#         # Store to global\n",
    "#         unique_site_all = unique_site_all + not_common_site.tolist()\n",
    "#         lenght_unique = not_common_site.shape[0]\n",
    "\n",
    "#         # Index of unique sites for each loop and store to global index value\n",
    "#         index_list = np.arange(start_index + 1, start_index + lenght_unique + 1)\n",
    "#         start_index = start_index + lenght_unique\n",
    "#         index_list_all = index_list_all + index_list.tolist() \n",
    "\n",
    "#         # Unique elements for each iteration\n",
    "#         count_index = map(lambda x: x, index_list)\n",
    "#         counts_sites = map(lambda x: unique_counts[x], index_not_common_site)\n",
    "#         freq_site = list( zip(count_index, counts_sites) )\n",
    "#         # Store to dictionary\n",
    "#         site_freq_users = dict(zip(unique_site[index_not_common_site].tolist(),freq_site))\n",
    "#         site_freq_users_all.update(site_freq_users)\n",
    "\n",
    "#         # Common unique elements for each iteration and store ti dictionary\n",
    "#         count_comon_index = [site_freq_users_all.get(i)[0] for i in common_site]\n",
    "#         counts_comon_sites = unique_counts[index_com_site].tolist()\n",
    "#         freq_common_site = list( zip(count_comon_index, counts_comon_sites) )\n",
    "#         site_freq_common_users = dict(zip(common_site, freq_common_site))\n",
    "#         site_freq_users_all.update(dict_modify(site_freq_users_all, site_freq_common_users))\n",
    "\n",
    "#         # Replacing index and raplace dictionary\n",
    "#         replasment = dict(zip(unique_site_all,index_list_all))\n",
    "    \n",
    "#         # User id value\n",
    "#         user_id = file + 1 \n",
    "#         # Copy of daraframe for main algorytm\n",
    "#         user_data_eval = user_data[file].copy() \n",
    "    \n",
    "#         # Condition for determining incomplete session\n",
    "#         incomplete_condition(user_data_eval, session_length)\n",
    "    \n",
    "#         # Main alhortm of replacing str sites for numbers\n",
    "#         slice_index = [*map(lambda x: (x[0] - 1, x[1] - 1),\\\n",
    "#                             window_session_items(session_length, window_length, user_data_eval['site'].shape[0]))]\n",
    "#         sessions_length = map(lambda x: x[1] - x[0] + 1, slice_index)\n",
    "#         sessions_range = [*map(np.arange, sessions_length)]\n",
    "\n",
    "#         all_sessions = [*map(lambda x: user_data_eval['site'].values[x[0]:x[1]+1], slice_index)]\n",
    "#         sessions = len([*window_session_items(session_length, window_length, user_data_eval['site'].shape[0])])\n",
    "\n",
    "#         resultData = pd.DataFrame(index=np.arange(sessions), columns=site_numeration)\n",
    "\n",
    "#         for session in np.arange(sessions):\n",
    "#             for index in sessions_range[session]:\n",
    "#                 resultData.iat[session,index] = all_sessions[session].tolist()[index]\n",
    "#         resultData = resultData.fillna(0)     \n",
    "\n",
    "#         # resultData add user id\n",
    "#         resultData['user_id'] = pd.Series(user_id, index=resultData.index)\n",
    "    \n",
    "#         # Store to the global value\n",
    "#         resultData_all = resultData_all.append(resultData, ignore_index=True)\n",
    "    \n",
    "#     #Replacing site_id in column and delete NANs\n",
    "#     for site in site_numeration:\n",
    "#         resultData_all[site] = resultData_all[site].map(replasment.get)\n",
    "#     resultData_all = resultData_all.fillna(0).astype(int)\n",
    "\n",
    "#     return resultData_all, site_freq_users_all"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
